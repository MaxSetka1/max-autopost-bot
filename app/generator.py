# app/generator.py
from __future__ import annotations

import os, json, re
from typing import Dict, List, Any

from app.retriever import search_book
from app.gpt import _client
from app.sheets import get_book_meta  # –∞–≤—Ç–æ—Ä/–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–∏—Å—Ç–∞ books

MODEL_SUMMARY = os.getenv("OPENAI_MODEL_SUMMARY", "gpt-4o-mini")
MODEL_POSTS   = os.getenv("OPENAI_MODEL_POSTS",   "gpt-4o-mini")

_SUMMARY_CACHE: Dict[str, Dict[str, Any]] = {}

# ---------- –¢–µ–∫—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã ----------
def _clean_bold(s: str) -> str:
    return s.replace("**", "").strip()

def _squash_blanks(s: str) -> str:
    return re.sub(r"\n{3,}", "\n\n", s).strip()

def _normalize(s: str) -> str:
    return _squash_blanks(_clean_bold(s))

def _deslug(s: str) -> str:
    # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è/–¥–µ—Ñ–∏—Å—ã, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r"\.(pdf|docx?|rtf|epub|txt)$", "", s, flags=re.I)
    s = s.replace("_", " ").replace("-", " ")
    s = re.sub(r"\s{2,}", " ", s).strip()
    return s

# –ê–Ω—Ç–∏-–∫–ª–∏–∫–±–µ–π—Ç (—Ñ–∏–∫—Å: —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –Ω–µ tuple)
_STOP_START = r"(?:–∑–Ω–∞–µ—Ç–µ –ª–∏ –≤—ã|–≤—ã –∑–Ω–∞–ª–∏|–∞ –∑–Ω–∞–µ—Ç–µ|–∞ —á—Ç–æ –µ—Å–ª–∏|—á—Ç–æ –µ—Å–ª–∏|—Å–µ–∫—Ä–µ—Ç –≤ —Ç–æ–º|–º–Ω–æ–≥–∏–µ –Ω–µ –∑–Ω–∞—é—Ç|–ø—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ|–ø—Ä–µ–¥—Å—Ç–∞–≤—å)"

def _declickbait(text: str) -> str:
    if not text:
        return text
    text = re.sub(r"(?im)^\s*" + _STOP_START + r".{0,120}\?\s*\n?", "", text)
    text = re.sub(r"(?im)–≤\s+–æ–¥–Ω–æ–π\s+–∏–∑\s+–≥–ª–∞–≤.*?(?:\.|\n)", "", text)
    text = re.sub(r"(?im)—ç—Ç–∞\s+–∫–Ω–∏–≥–∞\s+–ø–æ–∫–∞–∂–µ—Ç.*?(?:\.|\n)", "", text)
    # –î–æ–ø. —à—Ç–∞–º–ø–∏–∫–∏
    text = re.sub(r"(?im)–ø–æ–≥—Ä—É–∑–∏—Ç–µ—Å—å\s+–≤\s+–º–∏—Ä.*?(?:\.|\n)", "", text)
    text = re.sub(r"(?im)–æ—Ç–∫—Ä–æ–π—Ç–µ\s+–Ω–æ–≤—ã–µ\s+–≥–æ—Ä–∏–∑–æ–Ω—Ç—ã.*?(?:\.|\n)", "", text)
    return _squash_blanks(text)

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏
_EMOJI_RE = re.compile(
    r"[\U0001F1E6-\U0001F1FF]|"   # —Ñ–ª–∞–≥–∏
    r"[\U0001F300-\U0001F5FF]|"   # —Å–∏–º–≤–æ–ª—ã/–ø–∏–∫—Ç–æ–≥—Ä–∞–º–º—ã
    r"[\U0001F600-\U0001F64F]|"   # —Å–º–∞–π–ª–∏–∫–∏
    r"[\U0001F680-\U0001F6FF]|"   # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç/—Å–∏–º–≤–æ–ª—ã
    r"[\U00002600-\U000026FF]|"   # —Ä–∞–∑–Ω–æ–µ
    r"[\U00002700-\U000027BF]|"   # –ª–∏—Ç–µ—Ä–∞–ª—ã
    r"[\U0001FA70-\U0001FAFF]",   # —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    flags=re.UNICODE
)

def _limit_emojis(text: str, max_count: int) -> str:
    if max_count <= 0:
        return _EMOJI_RE.sub("", text)
    out, used = [], 0
    for ch in text:
        if _EMOJI_RE.fullmatch(ch):
            if used < max_count:
                out.append(ch); used += 1
        else:
            out.append(ch)
    return "".join(out)

# ---------- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—ã –∏–∑ –ª–∏—Å—Ç–∞ ----------
def _as_meta_dict(meta_like) -> Dict[str, Any]:
    # get_book_meta –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å dict –ò–õ–ò (dict, row)
    if isinstance(meta_like, tuple) and meta_like:
        meta_like = meta_like[0]
    return meta_like or {}

# ---------- –•–µ–ª–ø–µ—Ä—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–Ω–∏–≥ (fallback –Ω–∞ —Ä—É—Å—Å–∫–∏–π) ----------
def _norm_key(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"\.(pdf|docx?|rtf|epub|txt)$", "", s)
    s = s.replace(" ", "").replace("_", "").replace("-", "")
    s = s.replace("—ë", "–µ")
    return s

# –°–ª–æ–≤–∞—Ä—å –Ω–∞–∑–≤–∞–Ω–∏–π
_KNOWN_TITLES: Dict[str, str] = {
    _norm_key("atomnye privychki"): "–ê—Ç–æ–º–Ω—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏",
    _norm_key("atomnye-privychki"): "–ê—Ç–æ–º–Ω—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏",
    _norm_key("atomic habits"): "–ê—Ç–æ–º–Ω—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏",
    _norm_key("ot-nulya-k-edinice"): "–û—Ç –Ω—É–ª—è –∫ –µ–¥–∏–Ω–∏—Ü–µ",
    _norm_key("from zero to one"): "–û—Ç –Ω—É–ª—è –∫ –µ–¥–∏–Ω–∏—Ü–µ",
    _norm_key("7 navykov vysoo-effektivnyh lyudei"): "7 –Ω–∞–≤—ã–∫–æ–≤ –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ª—é–¥–µ–π",
    _norm_key("7 navykov"): "7 –Ω–∞–≤—ã–∫–æ–≤ –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ª—é–¥–µ–π",
    _norm_key("7 habits of highly effective people"): "7 –Ω–∞–≤—ã–∫–æ–≤ –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ª—é–¥–µ–π",
}

# –°–ª–æ–≤–∞—Ä—å –∞–≤—Ç–æ—Ä–æ–≤
_KNOWN_AUTHORS: Dict[str, str] = {
    _norm_key("–ê—Ç–æ–º–Ω—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏"): "–î–∂–µ–π–º—Å –ö–ª–∏—Ä",
    _norm_key("Atomic Habits"): "–î–∂–µ–π–º—Å –ö–ª–∏—Ä",
    _norm_key("–û—Ç –Ω—É–ª—è –∫ –µ–¥–∏–Ω–∏—Ü–µ"): "–ü–∏—Ç–µ—Ä –¢–∏–ª—å",
    _norm_key("From Zero to One"): "–ü–∏—Ç–µ—Ä –¢–∏–ª—å",
    _norm_key("7 –Ω–∞–≤—ã–∫–æ–≤ –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ª—é–¥–µ–π"): "–°—Ç–∏–≤–µ–Ω –ö–æ–≤–∏",
    _norm_key("7 Habits of Highly Effective People"): "–°—Ç–∏–≤–µ–Ω –ö–æ–≤–∏",
}

def _guess_title_from_slug(book_id: str, channel_name: str) -> str:
    key = _norm_key(book_id or channel_name)
    if key in _KNOWN_TITLES:
        return _KNOWN_TITLES[key]
    return _deslug(book_id or channel_name)

def _guess_author_by_title(title: str) -> str:
    key = _norm_key(title)
    return _KNOWN_AUTHORS.get(key, "")

# ---------- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ ----------
def _book_title(summary: Dict[str, Any], book_id: str, channel_name: str) -> str:
    """–ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏: –ª–∏—Å—Ç books ‚Üí JSON-–∫–æ–Ω—Å–ø–µ–∫—Ç ‚Üí —Å–ª–æ–≤–∞—Ä—å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö ‚Üí –æ—á–∏—â–µ–Ω–Ω—ã–π id/alias."""
    t = ""
    # 1) –õ–∏—Å—Ç books (–Ω–∞–¥—ë–∂–Ω–µ–µ –≤—Å–µ–≥–æ)
    try:
        meta = _as_meta_dict(get_book_meta(book_id))
        t = (meta.get("title") or "").strip()
    except Exception:
        t = ""
    # 2) –ò–∑ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞
    if not t:
        about = summary.get("about") or {}
        t = (about.get("title") or "").strip()
    # 3) –ò–∑ —Å–ª–æ–≤–∞—Ä—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ slug/id
    if not t:
        t = _guess_title_from_slug(book_id, channel_name)
    return t

def _book_author(summary: Dict[str, Any], book_id: str) -> str:
    """–ê–≤—Ç–æ—Ä ‚Äî –ª–∏—Å—Ç books ‚Üí JSON-–∫–æ–Ω—Å–ø–µ–∫—Ç ‚Üí –ø–æ —É–∑–Ω–∞–≤–∞–µ–º–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é."""
    a = ""
    # 1) –õ–∏—Å—Ç books
    try:
        meta = _as_meta_dict(get_book_meta(book_id))
        a = (meta.get("author") or "").strip()
    except Exception:
        a = ""
    # 2) –ö–æ–Ω—Å–ø–µ–∫—Ç
    if not a:
        about = summary.get("about") or {}
        a = (about.get("author") or "").strip()
    # 3) –ü–æ —É–∑–Ω–∞–≤–∞–µ–º–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é
    if not a:
        # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–≤–µ—Å—Ç–∏ –∞–≤—Ç–æ—Ä–∞ –∏–∑ –∑–Ω–∞–∫–æ–≤—ã—Ö –∫–Ω–∏–≥
        # –ë–µ—Ä—ë–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–∑ –≤—Å–µ—Ö –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        # (—á—Ç–æ–±—ã –Ω–µ –ø–ª–æ–¥–∏—Ç—å –≤—ã–∑–æ–≤—ã sheets)
        title_guess = (summary.get("about") or {}).get("title") or ""
        if not title_guess:
            title_guess = _guess_title_from_slug(book_id, "")
        a = _guess_author_by_title(title_guess) or ""
    return a

# ---------- –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ ----------
def _collect_context(book_id: str) -> str:
    queries = [
        "–æ—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∫–Ω–∏–≥–∏ –≤ —Ü–µ–ª–æ–º",
        "–∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏ –ø—Ä–∞–≤–∏–ª–∞ –∞–≤—Ç–æ—Ä–∞",
        "–ø–æ—à–∞–≥–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è",
        "–ø—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è",
        "—Å–∏–ª—å–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏",
        "–¥–ª—è –∫–æ–≥–æ –∫–Ω–∏–≥–∞ –∏ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã",
    ]
    chunks, seen = [], set()
    for q in queries:
        for ch in search_book(book_id, q, top_k=10):
            t = (ch.get("text") or "").strip()
            if t and t not in seen:
                seen.add(t); chunks.append(t)
            if len(chunks) >= 60:
                break
        if len(chunks) >= 60:
            break
    joined = "\n\n".join(chunks)
    return joined[:40_000] if len(joined) > 40_000 else joined

# ---------- –ö–æ–Ω—Å–ø–µ–∫—Ç (JSON) ----------
def _ask_json_summary(context: str, book_id: str, channel_name: str) -> Dict[str, Any]:
    system = "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –¥–µ–ª–æ–≤–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞. –°–¥–µ–ª–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–π –∫–æ–Ω—Å–ø–µ–∫—Ç –∫–Ω–∏–≥–∏. –†—É—Å—Å–∫–∏–π —è–∑—ã–∫."
    user = f"""
–ù–∞ –≤—Ö–æ–¥–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–Ω–∏–≥–∏. –°–¥–µ–ª–∞–π JSON-–∫–æ–Ω—Å–ø–µ–∫—Ç:

{{
  "about": {{"title":"","author":"","thesis":"","audience":""}},
  "key_ideas": ["..."],
  "practices": [{{"name":"","steps":["—à–∞–≥ 1","—à–∞–≥ 2"]}}],
  "cases": ["..."],
  "quotes": [{{"text":"","note":""}}],
  "reflection": ["..."]
}}

–¢–æ—á–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ñ–∞–Ω—Ç–∞–∑–∏–∏: –µ—Å–ª–∏ —Ñ–∞–∫—Ç–∞ –Ω–µ—Ç ‚Äî –æ—Å—Ç–∞–≤—å –ø–æ–ª–µ –ø—É—Å—Ç—ã–º.
–í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

–§—Ä–∞–≥–º–µ–Ω—Ç—ã:
---
{context}
---
"""
    resp = _client().chat.completions.create(
        model=MODEL_SUMMARY,
        messages=[{"role":"system","content":system},
                  {"role":"user","content":user}],
        temperature=0.2,
        response_format={"type":"json_object"},
    )
    try:
        return json.loads(resp.choices[0].message.content)
    except Exception:
        return {
            "about":{"title":"","author":"","thesis":"","audience":""},
            "key_ideas":[],"practices":[],"cases":[],"quotes":[],"reflection":[]
        }

def _ensure_summary(book_id: str, channel_name: str) -> Dict[str, Any]:
    if book_id in _SUMMARY_CACHE:
        return _SUMMARY_CACHE[book_id]
    ctx = _collect_context(book_id)
    summary = _ask_json_summary(ctx, book_id, channel_name)
    _SUMMARY_CACHE[book_id] = summary
    return summary

# ---------- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ ----------
def _gen_with_prompt(fmt: str, summary: Dict[str, Any], *, book_id: str, channel_name: str) -> str:
    base = json.dumps(summary, ensure_ascii=False, indent=2)
    title = _book_title(summary, book_id, channel_name)
    author = _book_author(summary, book_id)

    prompts = {
        "announce": (
            "–°–¥–µ–ª–∞–π –∞–Ω–æ–Ω—Å –∫–Ω–∏–≥–∏ –¥–ª—è Telegram.\n"
            "–°—Ç—Ä—É–∫—Ç—É—Ä–∞:\n"
            "- 1 —Å—Ç—Ä–æ–∫–∞: –∑–∞—á–µ–º —á–∏—Ç–∞—Ç—å (–±–µ–∑ —Å–ª–æ–≤ ¬´—ç—Ç–∞ –∫–Ω–∏–≥–∞ –ø–æ–∫–∞–∂–µ—Ç¬ª),\n"
            "- 2‚Äì3 –±—É–ª–ª–µ—Ç–∞: –∫–æ–º—É –ø–æ–ª–µ–∑–Ω–æ/–∫–∞–∫–æ–π –≤—ã–∏–≥—Ä—ã—à,\n"
            "- 1 –∫—Ä—é—á–æ–∫: —è—Ä–∫–∞—è —Ü–∏—Ñ—Ä–∞/—Ñ–∞–∫—Ç/–º–µ—Ç–∞—Ñ–æ—Ä–∞.\n"
            "–°—Ç–∏–ª—å: —ç–Ω–µ—Ä–≥–∏—á–Ω–æ, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, –±–µ–∑ –≤–æ–¥—ã, 2‚Äì3 —É–º–µ—Å—Ç–Ω—ã—Ö —ç–º–æ–¥–∑–∏.\n"
            "–ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –Ω–∞–∑–≤–∞–Ω–∏–µ ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–±–∞–≤–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ. –ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ (**)."
        ),
        "insight": (
            "–í—ã–¥–µ–ª–∏ 3‚Äì5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–¥–µ–π –∏–∑ –∫–Ω–∏–≥–∏. –ö–∞–∂–¥–∞—è ‚Äî 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è + –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–π –ø—Ä–∏–º–µ—Ä.\n"
            "–°—Ç–∏–ª—å: –ª–∞–∫–æ–Ω–∏—á–Ω–æ, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ, 1‚Äì2 —ç–º–æ–¥–∑–∏ —Å—É–º–º–∞—Ä–Ω–æ. –ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–±–∞–≤–∏–º —Å–∞–º–∏."
        ),
        "practice": (
            "–í–æ–∑—å–º–∏ 1 –ø—Ä–∏–∫–ª–∞–¥–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É. –î–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ 3‚Äì6 —á—ë—Ç–∫–∏—Ö —à–∞–≥–æ–≤.\n"
            "–î–æ–±–∞–≤—å –±—ã—Ç–æ–≤–æ–π –ø—Ä–∏–º–µ—Ä (–æ–¥–Ω–∏–º –∞–±–∑–∞—Ü–µ–º).\n"
            "–°—Ç–∏–ª—å: –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã, 1‚Äì2 —ç–º–æ–¥–∑–∏. –ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–±–∞–≤–∏–º —Å–∞–º–∏."
        ),
        "case": (
            "–û–ø–∏—à–∏ 1 –∫–µ–π—Å: –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí –¥–µ–π—Å—Ç–≤–∏–µ ‚Üí —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Üí –≤—ã–≤–æ–¥ (3‚Äì5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).\n"
            "–ë–µ–∑ –æ–±—â–∏—Ö —à—Ç–∞–º–ø–æ–≤, 0‚Äì1 —ç–º–æ–¥–∑–∏. –ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–±–∞–≤–∏–º —Å–∞–º–∏."
        ),
        "quote": (
            "–î–∞–π 1 —Å–∏–ª—å–Ω—É—é —Ü–∏—Ç–∞—Ç—É –¥–æ—Å–ª–æ–≤–Ω–æ –≤ –∫–∞–≤—ã—á–∫–∞—Ö + 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å.\n"
            "–ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ, 0‚Äì1 —ç–º–æ–¥–∑–∏. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–±–∞–≤–∏–º —Å–∞–º–∏."
        ),
        "reflect": (
            "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 1‚Äì2 –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ —Ç–∞–∫, —á—Ç–æ–±—ã —á–∏—Ç–∞—Ç–µ–ª—å –ø—Ä–∏–º–µ—Ä–∏–ª –∏–¥–µ—é –Ω–∞ —Å–µ–±—è.\n"
            "–ö–æ—Ä–æ—Ç–∫–æ, 0‚Äì1 —ç–º–æ–¥–∑–∏. –ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–±–∞–≤–∏–º —Å–∞–º–∏."
        ),
    }
    prompt = prompts.get(fmt, "–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–æ –∫–Ω–∏–≥–µ: –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, –±–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ –∏ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞.")

    resp = _client().chat.completions.create(
        model=MODEL_POSTS,
        messages=[
            {"role":"system","content":"–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä Telegram-–∫–∞–Ω–∞–ª–∞: –ø–∏—à–∏ —è—Ä–∫–æ, –ø–æ –¥–µ–ª—É, —Å –ª—ë–≥–∫–∏–º–∏ —ç–º–æ–¥–∑–∏ –∏ –±–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è."},
            {"role":"user","content":f"–ö–æ–Ω—Å–ø–µ–∫—Ç –∫–Ω–∏–≥–∏:\n{base}\n\n–ó–∞–¥–∞—á–∞:\n{prompt}"}
        ],
        temperature=0.7,
    )
    body = _normalize(resp.choices[0].message.content or "")
    body = _declickbait(body)

    # –ª–∏–º–∏—Ç —ç–º–æ–¥–∑–∏ –∏–∑ –ø.2: 1 / 2 / 3 –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã
    length = len(body)
    max_emoji = 1 if length < 400 else (2 if length < 800 else 3)
    body = _limit_emojis(body, max_emoji)

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏/—Ö—ç—à—Ç–µ–≥–∏
    map_emoji = {
        "announce": "üìö",
        "insight":  "üí°",
        "practice": "üõ†Ô∏è",
        "case":     "üìå",
        "quote":    "üó£Ô∏è",
        "reflect":  "üß≠",
    }
    map_tag = {
        "announce": "#–∞–Ω–æ–Ω—Å #–∫–Ω–∏–≥–∞",
        "insight":  "#–∏–Ω—Å–∞–π—Ç",
        "practice": "#–ø—Ä–∞–∫—Ç–∏–∫–∞",
        "case":     "#–∫–µ–π—Å",
        "quote":    "#—Ü–∏—Ç–∞—Ç–∞",
        "reflect":  "#—Ä–µ—Ñ–ª–µ–∫—Å–∏—è",
    }
    map_label = {
        "announce": "–∞–Ω–æ–Ω—Å",
        "insight":  "–∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏",
        "practice": "–ø—Ä–∞–∫—Ç–∏–∫–∞",
        "case":     "–∫–µ–π—Å",
        "quote":    "—Ü–∏—Ç–∞—Ç–∞",
        "reflect":  "–≤–æ–ø—Ä–æ—Å –¥–Ω—è",
    }

    emoji = map_emoji.get(fmt, "üìù")
    label = map_label.get(fmt, fmt)
    tags  = map_tag.get(fmt, "#—Å–≤–æ–¥–∫–∞")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫: –¥–ª—è –∞–Ω–æ–Ω—Å–∞ –¥–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ä–∞, –µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω
    if fmt == "announce":
        title_full = f"{title} ({author})" if author else title
        header = f"{emoji} –ö–Ω–∏–≥–∞ –¥–Ω—è ‚Äî {title_full}"
    else:
        header = f"{emoji} {title} ‚Äî {label.capitalize()}"

    final = f"{header}\n\n{body}\n\n{tags}"
    return final.strip()

# ---------- –ü—É–±–ª–∏—á–Ω—ã–µ ----------
def generate_from_book(channel_name: str, book_id: str, fmt: str) -> str:
    s = _ensure_summary(book_id, channel_name)
    return _gen_with_prompt(fmt.lower(), s, book_id=book_id, channel_name=channel_name)

def generate_by_format(fmt: str, items: List[dict]) -> str:
    f = (fmt or "").lower()
    if f == "quote":
        return "¬´–í—ã ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–≥–æ, —á—Ç–æ –¥–µ–ª–∞–µ—Ç–µ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å¬ª. #—Ü–∏—Ç–∞—Ç–∞"
    if f == "practice":
        return "–ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–µ–¥–µ–ª–∏: –ø—Ä–∞–≤–∏–ª–æ 2 –º–∏–Ω—É—Ç. #–ø—Ä–∞–∫—Ç–∏–∫–∞"
    return "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –≥–æ—Ç–æ–≤—è—Ç—Å—è. #—Å–≤–æ–¥–∫–∞"

def get_author_for_book(book_id: str, channel_name: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–≤—Ç–æ—Ä–∞ –∫–Ω–∏–≥–∏: —Å–ø–µ—Ä–≤–∞ –ª–∏—Å—Ç books, –∑–∞—Ç–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç, –∑–∞—Ç–µ–º —Å–ª–æ–≤–∞—Ä—å –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π."""
    summary = _ensure_summary(book_id, channel_name)
    return _book_author(summary, book_id)
